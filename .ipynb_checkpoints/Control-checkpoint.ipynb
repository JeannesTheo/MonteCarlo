{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from plot_utils import plot_blackjack_values, plot_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_probs(Q_s, epsilon, nA):\n",
    "    \"\"\" \n",
    "    Get the probability of taking the best known action according to epsilon.\n",
    "    Returns the policy for the Q value given\n",
    "    \"\"\"\n",
    "    policy_s = np.ones(nA) * epsilon / nA\n",
    "    best_a = np.argmax(Q_s)\n",
    "    policy_s[best_a] = 1 - epsilon + (epsilon / nA)\n",
    "    return policy_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_policy(Q,nA):\n",
    "    \"\"\"\n",
    "    returns the best actions for each Q value in the policy\n",
    "    \"\"\"\n",
    "    return dict((k,np.argmax(v)) for k, v in Q.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_Q(env, episode, Q, alpha, gamma):\n",
    "    \"\"\"\n",
    "    Calculate the new Q values for the actions taken in the given episode\n",
    "    returns the new Q policy\n",
    "    \"\"\"\n",
    "    \n",
    "    for s, a, r in episode:\n",
    "        first_occurence_idx = next(i for i,x in enumerate(episode) if x[0] == s)\n",
    "        G = sum([x[2]*(gamma**i) for i,x in enumerate(episode[first_occurence_idx:])])\n",
    "        Q[s][a] = Q[s][a] + alpha*(G - Q[s][a])\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play_game(env, Q, epsilon, nA):\n",
    "    \"\"\" generates an episode from following the epsilon-greedy policy \"\"\"\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        probs = get_probs(Q[state], epsilon, nA)\n",
    "        action = np.random.choice(np.arange(nA), p=probs) \\\n",
    "                                    if state in Q else env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mc_control(env, num_episodes):\n",
    "    \"\"\"\n",
    "    main methof. Iterates through episodes updating epsilon after each, retrieves the list of states, actions\n",
    "    and rewards from the last episode and use them to calculate the updated Q values\n",
    "    \"\"\"\n",
    "    epsilon = 1.0\n",
    "    eps_min = 0.01\n",
    "    decay = 0.9999\n",
    "    alpha = 0.001\n",
    "    gamma = 1.0\n",
    "    \n",
    "    nA = env.action_space.n\n",
    "    # initialize empty dictionary of arrays\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        #update epsilon\n",
    "        epislon = max(epsilon*decay, eps_min)\n",
    "        episode = play_game(env, Q, epsilon, nA)\n",
    "        Q = update_Q(env, episode, Q, alpha, gamma)\n",
    "    \n",
    "    policy = best_policy(Q, nA)\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, Q = mc_control(env, 500000)\n",
    "\n",
    "V = dict((k,np.max(v)) for k, v in Q.items())\n",
    "\n",
    "# plot the state-value function\n",
    "plot_blackjack_values(V)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
